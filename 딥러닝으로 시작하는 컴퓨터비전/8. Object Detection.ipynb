{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNd5URat3gx4qKkDAWy4vjW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 8-2. Image Classification vs Localization vs Object Detection"],"metadata":{"id":"Y1ypv5p2TwGv"}},{"cell_type":"markdown","source":["## Computer Vision 내의 Task 복습"],"metadata":{"id":"X_3w5kutG1ng"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1GbFm1U7iKqRtlK0yZok7sCWWzmIRxplP\">\n","\n","[Computer Vision 내의 Task]\n","\n","http://cs231n.stanford.edu/slides/2016/winter1516_lecture13.pdf\n"],"metadata":{"id":"HBnXCPgUJauc"}},{"cell_type":"markdown","source":["- Single Object\n","-- Image Classification: 단 하나의 대상의 true label이 무엇인지 분류합니다.\n","-- Localization: 단 하나의 대상의 위치를 가장 적절하게 지정하는 bounding box(좌표)를 찾습니다.\n","- Multiple Objects\n","-- Object Detection: 여러 개의 bounding box를 찾아 여러 대상의 위치를 가장 적절하게 지정하는 동시에 각 bounding box 내의 대상을 판별합니다.\n","Segmentation: 픽셀 단위별로 detection을 수행합니다. (특정 픽셀이 속한 대상 판별)\n","-- Semantic Segmentation\n","-- Instance Segmentation"],"metadata":{"id":"d7KuCFhFJ0dk"}},{"cell_type":"markdown","source":["## Localization (+ Classification) vs. Object Detection"],"metadata":{"id":"A6S_bfoAKD5R"}},{"cell_type":"markdown","source":["- Localization과 Detection 모두 대상의 위치를 bounding box로 지정하고, 해당 bounding box 내의 대상이 무엇인지를 판별합니다.\n","- Localization과 비교하면, Object Detection은 하나의 이미지 내의 여러 대상의 위치를 찾고 분류해야 하기 때문에 Localization에 비해 난이도가 높습니다.\n","- CNN 모델은 데이터의 feature를 잘 찾아내지만 대상의 위치를 잡기 어렵기 때문에 Object Detection에 CNN을 사용하는 것은 좋지 않습니다. 그렇다면 Object Detection에는 어떤 모델을 사용해야 할까요?"],"metadata":{"id":"hsMagdCrKKV5"}},{"cell_type":"markdown","source":["# 8-3. Object Detection 모델의 발전 과정"],"metadata":{"id":"Xu4-HcShTzBp"}},{"cell_type":"markdown","source":["## 모델 발전 과정"],"metadata":{"id":"25ghwzYzKaMo"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1UROtJHkwZaQjv0NNvB2CrtzMhPvxAMN6\">\n","\n","[Object Detection 모델의 발전]\n","\n","https://arxiv.org/pdf/1905.05055.pdf\n","\n","2012년의 AlexNet을 기점으로 딥러닝 기반의 object detection 모델이 많이 연구되었습니다.\n","\n","- Two-Stage Detector: 객체가 있을 것 같은 영역을 뽑는 region proposal 진행 → proposed된 region에 classification / bounding box 찾기를 수행하는 두 단계로 나누어져 있습니다. (RCNN 계열)\n","- One-Stage Detector: 위의 두 단계가 한번에 수행됩니다. (YOLO 계열)"],"metadata":{"id":"_T6Y9GMjLZlZ"}},{"cell_type":"markdown","source":["## Two-Stage Detector"],"metadata":{"id":"s4P4zZbIMJNi"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1cT_Pc0SzG5JcNT8jms_Q4Ltr-gtsPhgI\">\n","\n","\n","[Faster R-CNN]\n","\n","https://arxiv.org/pdf/1506.01497.pdf\n","\n","- Region Proposal을 먼저 진행하면서 이미지 내에 대상이 있을 법한 영역인 RoI(Region of Interest)를 찾아냅니다.\n","- 찾아낸 RoI 안에 있는 이미지를 classification 합니다.\n","- 두 단계로 나누어져서 느리지만, 정확도가 비교적 높다는 장점이 존재합니다."],"metadata":{"id":"XX1HRANsMQGC"}},{"cell_type":"markdown","source":["## One-Stage Detector"],"metadata":{"id":"UL8q1_zVMwba"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1GdsgGvlvo_xGTElCq8w2Y7Rqow7qtVYW\">\n","\n","[YOLOv1]\n","\n","https://openaccess.thecvf.com/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf\n","\n","- Region Proposal과 Classification을 동시에 수행합니다.\n","- RoI를 찾아내는 대신, 이미지 전체를 대상으로 Classification을 수행합니다.\n","- YOLO v1의 경우에는 전체 이미지를 특정 크기의 grid로 분할한 후, cell의 중심에 object가 있다고 판단되는 특정 cell에 대하여 classification을 수행합니다.\n","- 속도는 Two-Stage Detector보다 빠르지만 정확도는 상대적으로 떨어질 수 있습니다. (현재는 정확도가 많이 개선되었습니다.)\n","- 자율주행 자동차, 영상 등 real-time processing을 요구하는 태스트에는 One-Stage Detector가 자주 활용됩니다."],"metadata":{"id":"pelZ_osXMy1Y"}},{"cell_type":"markdown","source":["# 8-4. R-CNN 모델을 통해 Object Detection 이해하기"],"metadata":{"id":"WQWQTXIsT18s"}},{"cell_type":"markdown","source":["## R-CNN의 전체 구조"],"metadata":{"id":"-gXuSVxLNkKy"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1o2Qom8_pu79CZ3qLdZmZN-ptzfXY25sW\">\n","\n","[R-CNN 구조]\n","\n","https://arxiv.org/pdf/1311.2524v5.pdf\n"],"metadata":{"id":"9HnPOGoFO0Kf"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1BvbyNsdSFys-g4UIIeuY2lP8GpRjGuH5\">\n","\n","[R-CNN 구조]\n","\n","http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture11.pdf\n","\n","\n","- Region Proposal: 입력 이미지에 selective search 알고리즘을 적용하여 객체가\n","있을 만한 RoI(Region of Interest)의 후보 2천개를 추출합니다.\n","- Resize: 추출된 RoI의 후보 2천개의 크기를 227x227로 변형합니다. (동일한 사이즈로 변형하기 때문에 이미지의 왜곡이 있을 수 있습니다.)\n","- 이미 학습된 CNN 구조를 통해서 4,096차원의 특징 벡터를 추출합니다.\n","각각의 객체별로 학습된 SVM classifier를 이용해서, 추출된 특징 벡터를 분류합니다.\n","- Bounding box regression으로 적절한 객체의 경계(bounding box)를 설정합니다."],"metadata":{"id":"tc7ZTw48PETL"}},{"cell_type":"markdown","source":["## Region Proposal: Selective Search"],"metadata":{"id":"wmVuiMPiUl9f"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=12A60iFcdhZZz4pKWpAN_AVVIG-vIKjH0\">\n","\n","[Selective Search의 과정]\n","\n","http://vision.stanford.edu/teaching/cs231b_spring1415/slides/ssearch_schuyler.pdf"],"metadata":{"id":"bSbVAy3ePEwI"}},{"cell_type":"markdown","source":["### Selective Search의 과정"],"metadata":{"id":"G6_Rr93iVPhu"}},{"cell_type":"markdown","source":["- 색, 무늬 크기, 형태를 바탕으로 주변 픽셀 간의 유사도를 계산합니다.\n","- 유사도를 바탕으로 segmentation을 수행한 후, 작은 segment들을 묶어가며 최종 후보를 찾습니다.\n","- 초기 segmentation은 매우 세밀한 영역까지 segmentation하는 over-segmentation을 합니다.\n","- 유사도가 비슷한 segment들을 반복적으로 묶어갑니다."],"metadata":{"id":"13NsE8bGPFMB"}},{"cell_type":"markdown","source":["## Classification"],"metadata":{"id":"GRqUftKEVmFi"}},{"cell_type":"markdown","source":["- RoI를 동일한 사이즈로 맞춘 후, Pre-trained된 Convolutional Neural Network 모델을 통해서 feature extraction(4,096차원)을 수행합니다.\n","- Feature Extraction 결과 바탕으로 학습한 SVM을 이용해서 feature extraction 결과를 분류합니다.\n","- 2,000개의 proposed region 중에서 IoU 값을 이용해 \"non-maximum suppression\"을 적용해 적합하지 않은 것을 탈락시킵니다.\n","- Bounding box의 위치를 맞추기 위해서 bounding box regression을 실행합니다.\n","- R-CNN은 Region proposal과 Classification과정이 분리되어 진행되므로 End-to-End가 아닙니다."],"metadata":{"id":"UdYWCq9dVte3"}},{"cell_type":"markdown","source":["## Object Detection과 관련된 개념들"],"metadata":{"id":"zIroFyWkV8Pi"}},{"cell_type":"markdown","source":["### Sliding window"],"metadata":{"id":"zWBTZAtvV-Hc"}},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=17lNOkZ6zBm_hp2NZT9cTk-kPO_aDyY_n\">\n","\n","sliding window(출처:https://towardsdatascience.com/going-deep-into-object-detection-bed442d92b34)\n","\n","Object Detection은 이미지의 “어느 위치”에 Object가 있는지 알아보는 태스크입니다.\n","\n","Sliding window는 일정 크기의 window를 이미지 위에서 조금씩 옮겨가며 전수조사를 하는 것입니다. Window 사이즈를 바꿔 가면서 Object가 있는 위치를 찾고, 효율적으로 Object 위치를 찾기 위해서 stride를 변경할 수 있습니다. 그러나 계산 비용이 많이 들고 학습 속도가 느리다는 단점이 있습니다."],"metadata":{"id":"_2cz5saRWDY1"}},{"cell_type":"markdown","source":["### IoU (Intersection over Union)"],"metadata":{"id":"auiZJsqDXab0"}},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=1L6ihKaywDIwWbM8WGPXmynN3QLgj7alZ\">\n","\n","[IoU]\n","\n","https://medium.com/the-research-nest/parking-space-detection-using-deep-learning-9fc99a63875e\n","\n","IoU는 모델이 예측한 bounding box와 실제 정답인 ground truth box가 얼마나 겹치는 지를 측정하는 지표입니다. 만약 100%로 겹치게 되면 IoU 값은 1이 됩니다.\n","\n","- Area of Union: predicted bounding box와 ground-truth bounding box를 모두 포함하는 영역\n","- Area of Overlap: predicted bounding box와 ground-truth bounding box가 겹치는 부분"],"metadata":{"id":"6OGYbbpZXmJJ"}},{"cell_type":"markdown","source":["### NMS (Non Maximum/maximal Suppression)"],"metadata":{"id":"F35yqZy3YOb6"}},{"cell_type":"markdown","source":["<img src = \"https://drive.google.com/uc?id=13j8wJhbxyDCpMz6VBeQ3GH7GJkudROZp\">\n","\n","[NMS]\n","\n","https://www.researchgate.net/figure/Non-Maximal-Suppression_fig5_345061606\n","NMS은 수많은 bounding box 중 가장 적합한 box를 선택하는 기법입니다.\n","\n"],"metadata":{"id":"kIOp9vDPYXWv"}},{"cell_type":"markdown","source":["- NMS의 과정\n","- 모든 bounding box에 대하여 threshold 이하의 confidence score를 가지는 bounding box는 제거합니다.\n","- 남은 bounding box들을 confidence score 기준으로 내림차순 정렬합니다.\n","정렬 후 가장 confidence score가 높은 bounding box를 기준으로 다른 bounding box와 IoU를 구합니다.\n","- IoU가 특정 기준 값보다 높으면, confidence score가 낮은 bounding box를 제거합니다.\n","- 해당 과정을 순차적으로 반복합니다."],"metadata":{"id":"6kBgy2VdY2Mj"}},{"cell_type":"markdown","source":["## mAP (mean Average Precision)"],"metadata":{"id":"CzYRHZG4ZXRC"}},{"cell_type":"markdown","source":["\n","<img src = \"https://drive.google.com/uc?id=14k5QKrMLFdxyIsTChhNysWrIX4WqlpnQ\">\n","\n","[Precision-Recall Curve]\n","\n","https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n","\n","- Precision-Recall Curve: confidence threshold의 변화에 따른 정밀도와 재현율의 변화 곡선입니다.\n","- AP: Precision-Recall Curve의 아래 부분 면적을 의미합니다.\n","- mAP: AP는 하나의 object에 대한 성능 수치이며, mAP는 여러 object들의 AP를 평균한 값을 의미합니다. 따라서 Object Detection 모델의 성능 평가에 사용합니다."],"metadata":{"id":"uYifFJ9wZY8Z"}},{"cell_type":"markdown","source":["### Bounding Box Regression"],"metadata":{"id":"9VSS-Y64bXvr"}},{"cell_type":"markdown","source":["\n","<img src = \"https://drive.google.com/uc?id=1Zi0u9o4UsxAW4FQjWez1OUc2kc7RCz5W\">\n","\n","[Bounding box regression]\n","\n","https://lilianweng.github.io/lil-log/2017/12/31/object-recognition-for-dummies-part-3.html\n","\n","Bounding box regression의 목표는 예측 박스(\n","�\n","𝑥\n",",\n","�\n","𝑦\n",",\n","�\n","𝑤\n",",\n","�\n","ℎ\n","p\n","x\n","​\n"," ,p\n","y\n","​\n"," ,p\n","w\n","​\n"," ,p\n","h\n","​\n"," )를 Ground truth box(\n","�\n","𝑥\n",",\n","�\n","𝑦\n",",\n","�\n","𝑤\n",",\n","�\n","ℎ\n","g\n","x\n","​\n"," ,g\n","y\n","​\n"," ,g\n","w\n","​\n"," ,g\n","h\n","​\n"," )에 가깝게 만드는 것입니다. 더 자세한 내용은 참고 자료를 통해서 확인해 보세요.\n","\n"," https://lilianweng.github.io/posts/2017-12-31-object-recognition-part-3/"],"metadata":{"id":"69tGgMKEbgXo"}},{"cell_type":"markdown","source":["# 학습정리"],"metadata":{"id":"qAWEZ4PXOVrp"}},{"cell_type":"markdown","source":["- Object detection은 한 이미지 안에 여러 개의 객체(multiple objects)가 있을 때, 객체의 경계를 지정(bounding box regression)해주고, 경계 안에 있는 객체의 class가 무엇인지를 분류하는 작업을 동시에 수행하는 것입니다.\n","- Object detection 모델은 2-stage detector와 1-stage detector로 구분됩니다."],"metadata":{"id":"m6LtrQMCOc0j"}}]}