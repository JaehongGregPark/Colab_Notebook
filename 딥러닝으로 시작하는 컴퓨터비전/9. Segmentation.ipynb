{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMLAoKxDhZOIx245x+aCxWj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 9-2. Semantic Segmentation vs Instance Segmentation"],"metadata":{"id":"ZmStiQu9UKiW"}},{"cell_type":"markdown","source":["## Segmentation에 대한 이해"],"metadata":{"id":"ltqObFTzdFOS"}},{"cell_type":"markdown","source":["<img src=\"https://drive.google.com/uc?id=1wZlBAkJSRbkrjfVHBV8YCKIp77Mkhwt8\">\n","\n","[segmentation]\n","\n","https://towardsdatascience.com/review-deepmask-instance-segmentation-30327a072339\n","\n","segmentation은 이미지를 픽셀 단위로 나누어서 특정 픽셀이 무엇을 지칭하는지를 파악하는 것입니다.\n"],"metadata":{"id":"NGHM4q3vdHex"}},{"cell_type":"markdown","source":["## Semantic Segmentation vs. Instance Segmentation"],"metadata":{"id":"I8nGLROQlB9C"}},{"cell_type":"markdown","source":["- semantic segmentation: 하나의 이미지 안에 들어있는 객체의 종류(object category)를 픽셀 단위로 찾습니다.\n","- instance segmentation: 하나의 이미지 안에 들어있는 객체의 개체(object instance)를 픽셀 단위로 찾습니다.\n","\n","instance segmentation = semantic segmentation + \"distinguishing instances\"\n","\n","이미지 데이터를 segmentation 모델 학습에 사용하기 위해서는 픽셀 하나하나 labeling을 해줘야 하기 때문에 데이터셋 구축이 어렵고, 따라서 data augmentation이 매우 중요합니다."],"metadata":{"id":"mgVw74KNlDko"}},{"cell_type":"markdown","source":["# 9-3. U-Net 구조를 통해서 Segmentation 이해하기"],"metadata":{"id":"6q7-pm0EUL5T"}},{"cell_type":"markdown","source":["## Semantic segmentation의 목표"],"metadata":{"id":"sHeok0oblWyc"}},{"cell_type":"markdown","source":["semantic segmentation의 목표는 이미지가 주어졌을 때, 픽셀 단위로 classification을 수행하여 이미지와 동일한 높이와 너비를 가진 segmentation map을 생성하는 것입니다. (아래 사진의 경우 5개의 label 존재)\n","\n","<img src=\"https://drive.google.com/uc?id=1IE19oxhNb76cFAEKevBNBhGU_DzJNP-H\">\n","\n","[semantic segmentation]\n","\n","https://www.jeremyjordan.me/semantic-segmentation/\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1eXfl_yyTMmcpTCMt395jQbgMqNSTkhps\">\n","\n","[semantic segmentation]\n","\n","https://www.jeremyjordan.me/semantic-segmentation/\n","\n","N개의 클래스에 대해서 semantic segmentation을 진행한다면 N개의 segmentation map(각각은 binary encoding)이 만들어져야 합니다.\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1N6cQ2vKF82hjAVcDpIWj4slw1LZVxJa6\">\n","\n","https://dacon.io/competitions/official/235591/overview/description\n","\n","output이 실수값이라면 regression task입니다. 기본적으로 segmentation은 분류 태스크이지만 segmentation 모델을 통해서 regression 태스크도 수행할 수 있습니다. 즉 위와 같은 인공위성 데이터를 이용한 강우량 예측은 semantic segmentation이지만 regression입니다. 이에 대한 자세한 사항은 링크를 통해 알아보세요."],"metadata":{"id":"crVyv1F2lxmJ"}},{"cell_type":"markdown","source":["## U-Net 모델 대략적으로 살펴보기"],"metadata":{"id":"bgQ0ajemozJ9"}},{"cell_type":"markdown","source":["- U-Net: Convolutional Networks for Biomedical Image Segmentation"],"metadata":{"id":"OffPTfayrKFa"}},{"cell_type":"markdown","source":["### Encoder-Decoder 모델"],"metadata":{"id":"lVO37mj4rQsG"}},{"cell_type":"markdown","source":["\n","<img src=\"https://drive.google.com/uc?id=1hxs4waBKx3A2wmBCc0l1PbXS1tgrfwop\">\n","\n","[Encoder-Decoder 모델]\n","\n","https://hackernoon.com/autoencoders-deep-learning-bits-1-11731e200694\n","\n","- Encoder와 Decoder로 구성되어 있습니다.\n","- 이전 노드에서 배웠던 Transposed Convolution을 사용합니다.\n","- 입력 이미지와 결과값 모두 3개의 채널을 가집니다. (RGB)\n","- Encoder에서는 convolution 연산을 수행하고, Decoder에서는 transposed convolution 연산을 수행합니다.\n","- U-Net: Encoder-Decoder 모델에 skip connection을 추가한 모델!"],"metadata":{"id":"XZHNr0dzrXkR"}},{"cell_type":"markdown","source":["## U-Net 모델 속 연산 확인하기"],"metadata":{"id":"RHgcGB7EsBly"}},{"cell_type":"markdown","source":["### Contracting path"],"metadata":{"id":"vYqqxs8lsDQc"}},{"cell_type":"markdown","source":["- 3 x 3 convolution 2번\n","\n","- Padding 사용하지 않음\n","\n","- ReLU activation\n","\n","- 2 x 2 max pooling\n","\n","- Down sampling 후 convolution channel size가 2배"],"metadata":{"id":"RSohQLprsIJe"}},{"cell_type":"markdown","source":["### Expanding (Expansive) path"],"metadata":{"id":"pyFs49NSsdTW"}},{"cell_type":"markdown","source":["- 2 x 2 up-convolution\n","- up-convolution (up-sampling) 후 channel의 수가 1/2이 되고, feature map의 size는 늘어남\n","- Cropped된 feature map과 concatenation\n","- 마지막 layer에 1x1 convolution 연산\n","자세한 설명은 영상을 통해 확인해 보세요. 🙂\n","\n","<img src = \"https://drive.google.com/uc?id=1pcAwlXnZ6lVwKDkN3UYY3HEiuAB2g6HV\">\n","\n","https://arxiv.org/pdf/1505.04597.pdf"],"metadata":{"id":"f47gkJMzslEm"}},{"cell_type":"markdown","source":["### Contracting path"],"metadata":{"id":"44sdy4mrtrUO"}},{"cell_type":"markdown","source":["convolution 연산으로 이루어진 부분 (위 그림의 왼쪽 절반, Encoder에 해당됩니다.)\n","\n","- CNN 구조와 유사합니다.\n","- 3x3 kernel을 사용하는 VGG 모델과 매우 유사합니다.\n","- 입력 이미지가 가지고 있는 context 정보를 추출합니다.\n","- 이미지의 위치에 대한 정보가 차츰 사라집니다."],"metadata":{"id":"j02xG34kttWi"}},{"cell_type":"markdown","source":["### Expanding (Expansive) path"],"metadata":{"id":"L5KH6BRxt9kB"}},{"cell_type":"markdown","source":["up-convolution 연산으로 이루어진 부분 (위 그림의 오른쪽 절반, Decoder에 해당됩니다.)\n","\n","- low resolution의 latent representation을 high resolution으로 변형합니다.\n","- contracting path에서 만들어진 feature map을 cropping한 결과물이 concatenation 됩니다.\n","- 원본 이미지가 가지고 있었던 위치 정보가 복원됩니다."],"metadata":{"id":"ixKMqAjOuEFz"}},{"cell_type":"markdown","source":["### Skip connection"],"metadata":{"id":"3iWkk3kduNzR"}},{"cell_type":"markdown","source":["- decoding 단계에서, 저차원의 정보와 고차원의 정보도 함께 이용하기 위한 방법입니다.\n","- encoding 과정에서의 정보 손실을 보충합니다."],"metadata":{"id":"FNFPQXlMuejd"}},{"cell_type":"markdown","source":["# 9-4. U-Net 코드를 통해서 이해 다지기"],"metadata":{"id":"CK-5bcI2URit"}},{"cell_type":"markdown","source":["## U-Net 코드 살펴보기"],"metadata":{"id":"xhgufVHxuts4"}},{"cell_type":"markdown","source":["U-Net 모델의 코드를 보면서 구조를 이해해봅시다.\n","\n","먼저 시각화에 필요한 라이브러리들을 설치하고 필요한 라이브러리를 불러옵니다."],"metadata":{"id":"DIgJLU0Du0ae"}},{"cell_type":"code","source":["!pip install graphviz\n","!pip install pydot"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqSaA0DSuwJl","executionInfo":{"status":"ok","timestamp":1711607126837,"user_tz":-540,"elapsed":20797,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"fc0ed012-8143-44d2-db99-c41430a636ce"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (0.20.3)\n","Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (1.4.2)\n","Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.10/dist-packages (from pydot) (3.1.2)\n"]}]},{"cell_type":"code","source":["import tensorflow.keras.layers as layers\n","import tensorflow as tf"],"metadata":{"id":"BSsdY7u3vCke","executionInfo":{"status":"ok","timestamp":1711607139558,"user_tz":-540,"elapsed":4069,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["U-Net 모델을 구현한 코드입니다.\n","\n","<img src=\"https://drive.google.com/uc?id=1VkKkoJo2W3RqfhX9WlObh0nuVLdPPGHm\">\n","\n","[Contracting Path]\n","\n","\n","<img src=\"https://drive.google.com/uc?id=1VkKkoJo2W3RqfhX9WlObh0nuVLdPPGHm\">\n","\n","[Expanding Path]\n","\n"],"metadata":{"id":"7cqMJHI0vKO4"}},{"cell_type":"code","source":["inputs = layers.Input(shape=(572, 572, 1))\n","\n","# Contracting path 시작\n","# [1]\n","conv0 = layers.Conv2D(64, activation='relu', kernel_size = 3)(inputs)\n","conv1 = layers.Conv2D(64, activation='relu', kernel_size=3)(conv0)  # Skip connection으로 Expanding path로 이어질 예정\n","conv2 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv1)\n","\n","# [2]\n","conv3 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv2)\n","conv4 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv3)  # Skip connection으로 Expanding path로 이어질 예정\n","conv5 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv4)\n","\n","# [3]\n","conv6 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv5)\n","conv7 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv6)  # Skip connection으로 Expanding path로 이어질 예정\n","conv8 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv7)\n","\n","# [4]\n","conv9 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv8)\n","conv10 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv9)  # Skip connection으로 Expanding path로 이어질 예정\n","conv11 = layers.MaxPool2D(pool_size=(2, 2), strides=(2, 2))(conv10)\n","\n","# [5]\n","conv12 = layers.Conv2D(1024, activation='relu', kernel_size=3)(conv11)\n","conv13 = layers.Conv2D(1024, activation='relu', kernel_size=3)(conv12)\n","# Contracting path 끝\n","\n","# Expanding path 시작\n","# [6]\n","trans01 = layers.Conv2DTranspose(512, kernel_size=2, strides=(2, 2), activation='relu')(conv13)\n","crop01 = layers.Cropping2D(cropping=(4, 4))(conv10)\n","concat01 = layers.concatenate([trans01, crop01], axis=-1)\n","\n","# [7]\n","conv14 = layers.Conv2D(512, activation='relu', kernel_size=3)(concat01)\n","conv15 = layers.Conv2D(512, activation='relu', kernel_size=3)(conv14)\n","trans02 = layers.Conv2DTranspose(256, kernel_size=2, strides=(2, 2), activation='relu')(conv15)\n","\n","# [8]\n","crop02 = layers.Cropping2D(cropping=(16, 16))(conv7)\n","concat02 = layers.concatenate([trans02, crop02], axis=-1)\n","\n","# [9]\n","conv16 = layers.Conv2D(256, activation='relu', kernel_size=3)(concat02)\n","conv17 = layers.Conv2D(256, activation='relu', kernel_size=3)(conv16)\n","trans03 = layers.Conv2DTranspose(128, kernel_size=2, strides=(2, 2), activation='relu')(conv17)\n","\n","# [10]\n","crop03 = layers.Cropping2D(cropping=(40, 40))(conv4)\n","concat03 = layers.concatenate([trans03, crop03], axis=-1)\n","\n","# [11]\n","conv18 = layers.Conv2D(128, activation='relu', kernel_size=3)(concat03)\n","conv19 = layers.Conv2D(128, activation='relu', kernel_size=3)(conv18)\n","trans04 = layers.Conv2DTranspose(64, kernel_size=2, strides=(2, 2), activation='relu')(conv19)\n","\n","# [12]\n","crop04 = layers.Cropping2D(cropping=(88, 88))(conv1)\n","concat04 = layers.concatenate([trans04, crop04], axis=-1)\n","\n","# [13]\n","conv20 = layers.Conv2D(64, activation='relu', kernel_size=3)(concat04)\n","conv21 = layers.Conv2D(64, activation='relu', kernel_size=3)(conv20)\n","# Expanding path 끝\n","outputs = layers.Conv2D(2, kernel_size=1)(conv21)\n","\n","model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"u-netmodel\")"],"metadata":{"id":"_l99nuhTvLTI","executionInfo":{"status":"ok","timestamp":1711607410733,"user_tz":-540,"elapsed":1555,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76im4ppQwLfX","executionInfo":{"status":"ok","timestamp":1711607434673,"user_tz":-540,"elapsed":679,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"9305be36-4277-4efe-eba4-a444ce045686"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"u-netmodel\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," input_1 (InputLayer)        [(None, 572, 572, 1)]        0         []                            \n","                                                                                                  \n"," conv2d (Conv2D)             (None, 570, 570, 64)         640       ['input_1[0][0]']             \n","                                                                                                  \n"," conv2d_1 (Conv2D)           (None, 568, 568, 64)         36928     ['conv2d[0][0]']              \n","                                                                                                  \n"," max_pooling2d (MaxPooling2  (None, 284, 284, 64)         0         ['conv2d_1[0][0]']            \n"," D)                                                                                               \n","                                                                                                  \n"," conv2d_2 (Conv2D)           (None, 282, 282, 128)        73856     ['max_pooling2d[0][0]']       \n","                                                                                                  \n"," conv2d_3 (Conv2D)           (None, 280, 280, 128)        147584    ['conv2d_2[0][0]']            \n","                                                                                                  \n"," max_pooling2d_1 (MaxPoolin  (None, 140, 140, 128)        0         ['conv2d_3[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_4 (Conv2D)           (None, 138, 138, 256)        295168    ['max_pooling2d_1[0][0]']     \n","                                                                                                  \n"," conv2d_5 (Conv2D)           (None, 136, 136, 256)        590080    ['conv2d_4[0][0]']            \n","                                                                                                  \n"," max_pooling2d_2 (MaxPoolin  (None, 68, 68, 256)          0         ['conv2d_5[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_6 (Conv2D)           (None, 66, 66, 512)          1180160   ['max_pooling2d_2[0][0]']     \n","                                                                                                  \n"," conv2d_7 (Conv2D)           (None, 64, 64, 512)          2359808   ['conv2d_6[0][0]']            \n","                                                                                                  \n"," max_pooling2d_3 (MaxPoolin  (None, 32, 32, 512)          0         ['conv2d_7[0][0]']            \n"," g2D)                                                                                             \n","                                                                                                  \n"," conv2d_8 (Conv2D)           (None, 30, 30, 1024)         4719616   ['max_pooling2d_3[0][0]']     \n","                                                                                                  \n"," conv2d_9 (Conv2D)           (None, 28, 28, 1024)         9438208   ['conv2d_8[0][0]']            \n","                                                                                                  \n"," conv2d_transpose (Conv2DTr  (None, 56, 56, 512)          2097664   ['conv2d_9[0][0]']            \n"," anspose)                                                                                         \n","                                                                                                  \n"," cropping2d (Cropping2D)     (None, 56, 56, 512)          0         ['conv2d_7[0][0]']            \n","                                                                                                  \n"," concatenate (Concatenate)   (None, 56, 56, 1024)         0         ['conv2d_transpose[0][0]',    \n","                                                                     'cropping2d[0][0]']          \n","                                                                                                  \n"," conv2d_10 (Conv2D)          (None, 54, 54, 512)          4719104   ['concatenate[0][0]']         \n","                                                                                                  \n"," conv2d_11 (Conv2D)          (None, 52, 52, 512)          2359808   ['conv2d_10[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_1 (Conv2D  (None, 104, 104, 256)        524544    ['conv2d_11[0][0]']           \n"," Transpose)                                                                                       \n","                                                                                                  \n"," cropping2d_1 (Cropping2D)   (None, 104, 104, 256)        0         ['conv2d_5[0][0]']            \n","                                                                                                  \n"," concatenate_1 (Concatenate  (None, 104, 104, 512)        0         ['conv2d_transpose_1[0][0]',  \n"," )                                                                   'cropping2d_1[0][0]']        \n","                                                                                                  \n"," conv2d_12 (Conv2D)          (None, 102, 102, 256)        1179904   ['concatenate_1[0][0]']       \n","                                                                                                  \n"," conv2d_13 (Conv2D)          (None, 100, 100, 256)        590080    ['conv2d_12[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_2 (Conv2D  (None, 200, 200, 128)        131200    ['conv2d_13[0][0]']           \n"," Transpose)                                                                                       \n","                                                                                                  \n"," cropping2d_2 (Cropping2D)   (None, 200, 200, 128)        0         ['conv2d_3[0][0]']            \n","                                                                                                  \n"," concatenate_2 (Concatenate  (None, 200, 200, 256)        0         ['conv2d_transpose_2[0][0]',  \n"," )                                                                   'cropping2d_2[0][0]']        \n","                                                                                                  \n"," conv2d_14 (Conv2D)          (None, 198, 198, 128)        295040    ['concatenate_2[0][0]']       \n","                                                                                                  \n"," conv2d_15 (Conv2D)          (None, 196, 196, 128)        147584    ['conv2d_14[0][0]']           \n","                                                                                                  \n"," conv2d_transpose_3 (Conv2D  (None, 392, 392, 64)         32832     ['conv2d_15[0][0]']           \n"," Transpose)                                                                                       \n","                                                                                                  \n"," cropping2d_3 (Cropping2D)   (None, 392, 392, 64)         0         ['conv2d_1[0][0]']            \n","                                                                                                  \n"," concatenate_3 (Concatenate  (None, 392, 392, 128)        0         ['conv2d_transpose_3[0][0]',  \n"," )                                                                   'cropping2d_3[0][0]']        \n","                                                                                                  \n"," conv2d_16 (Conv2D)          (None, 390, 390, 64)         73792     ['concatenate_3[0][0]']       \n","                                                                                                  \n"," conv2d_17 (Conv2D)          (None, 388, 388, 64)         36928     ['conv2d_16[0][0]']           \n","                                                                                                  \n"," conv2d_18 (Conv2D)          (None, 388, 388, 2)          130       ['conv2d_17[0][0]']           \n","                                                                                                  \n","==================================================================================================\n","Total params: 31030658 (118.37 MB)\n","Trainable params: 31030658 (118.37 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["위에서 구현한 U-Net 모델의 구조를 그림으로 나타낼 수 있습니다."],"metadata":{"id":"qWFuKllmwQoG"}},{"cell_type":"code","source":["from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot\n","\n","%matplotlib inline\n","\n","SVG(model_to_dot(model, show_shapes= True, show_layer_names=True, dpi=80).create(prog='dot', format='svg'))  #dpi를 작게 하면 그래프가 커집니다."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":401},"id":"rAGSJDDawLb-","executionInfo":{"status":"error","timestamp":1711607508600,"user_tz":-540,"elapsed":411,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"9b7b9ecc-be67-4c6e-badb-fbd90ae063bd"},"execution_count":6,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'keras.utils.vis_utils'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-ff9a7092c45a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSVG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvis_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodel_to_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.utils.vis_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","source":["## Transposed Convolution을 코드로 살펴보기"],"metadata":{"id":"H5FJSMz-wrrV"}},{"cell_type":"markdown","source":["Transposed Convolution을 코드를 통해 살펴 봅시다. 먼저 필요한 모듈을 불러옵니다."],"metadata":{"id":"-1ezioncwxeP"}},{"cell_type":"code","source":["# 필요한 모듈 불러오기\n","import numpy as np\n","import tensorflow as tf"],"metadata":{"id":"dNvCG3TQwLYe","executionInfo":{"status":"ok","timestamp":1711607613237,"user_tz":-540,"elapsed":450,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["input 데이터를 만들고 모델에 맞게 reshaping합니다."],"metadata":{"id":"Lg6_ZPfXxGXw"}},{"cell_type":"code","source":["# input data\n","X = np.asarray([[1, 2],\n","\t\t\t  [3, 4]])"],"metadata":{"id":"dnMlzfIKxLk5","executionInfo":{"status":"ok","timestamp":1711607696285,"user_tz":-540,"elapsed":363,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["print(X)\n","print(X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YcQVNGP3xQcb","executionInfo":{"status":"ok","timestamp":1711607716695,"user_tz":-540,"elapsed":284,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"64296718-90a7-438c-eb97-b4e959a14b37"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1 2]\n"," [3 4]]\n","(2, 2)\n"]}]},{"cell_type":"code","source":["# 모델에 맞게 reshaping\n","X = X.reshape((1, 2, 2, 1))"],"metadata":{"id":"75CIjh-exQZW","executionInfo":{"status":"ok","timestamp":1711607738111,"user_tz":-540,"elapsed":326,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["print(X)\n","print(X.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"goqlPL6UxQWS","executionInfo":{"status":"ok","timestamp":1711607760452,"user_tz":-540,"elapsed":355,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"68ece82e-60d3-40d0-ca74-f27fdb678653"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[[[[1]\n","   [2]]\n","\n","  [[3]\n","   [4]]]]\n","(1, 2, 2, 1)\n"]}]},{"cell_type":"markdown","source":["Transposed convolution 모델을 만들어 봅시다. 커널 사이즈 중 (1, 1) 부분을 다양하게 바꿔 보세요."],"metadata":{"id":"9iXRoNKfxh0N"}},{"cell_type":"code","source":["# 모델 만들기\n","model = tf.keras.models.Sequential()\n","model.add(tf.keras.layers.Conv2DTranspose(1, (1, 1), strides=(2, 2), input_shape=(2, 2, 1))) # Conv2DTranspos layer\n","# 다양한 커널 사이즈로 바꿔보세요."],"metadata":{"id":"6hxINWTtxQSx","executionInfo":{"status":"ok","timestamp":1711607821368,"user_tz":-540,"elapsed":284,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":["weights를 설정하고 설정한 weights를 모델에 적용합니다."],"metadata":{"id":"ZEylNWKNxvry"}},{"cell_type":"code","source":["weights = [np.asarray([[[[1]]]]), np.asarray([1])] # weight = 1, bias = 1"],"metadata":{"id":"AG9uG7XYxQK0","executionInfo":{"status":"ok","timestamp":1711607858852,"user_tz":-540,"elapsed":274,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9JX9Uuy_x0yC","executionInfo":{"status":"ok","timestamp":1711607875472,"user_tz":-540,"elapsed":279,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"5b083d5b-c4bc-4946-d286-7b2677163d52"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[[[1]]]]), array([1])]"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["model.set_weights(weights)"],"metadata":{"id":"9D2n9327x0u4","executionInfo":{"status":"ok","timestamp":1711607896676,"user_tz":-540,"elapsed":253,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["yhat = model.predict(X)\n","yhat = yhat.reshape((4, 4)) # 결과를 확인하기 편하게 reshaping\n","print(yhat)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aGO0UbfLx0rJ","executionInfo":{"status":"ok","timestamp":1711607914296,"user_tz":-540,"elapsed":693,"user":{"displayName":"Jaehong Park","userId":"07591456356612801440"}},"outputId":"2ddad358-97f2-4f86-a543-ebc11e91bb9f"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 211ms/step\n","[[2. 1. 3. 1.]\n"," [1. 1. 1. 1.]\n"," [4. 1. 5. 1.]\n"," [1. 1. 1. 1.]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"rXfdJwjSx0nI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 학습정리\n"],"metadata":{"id":"SnABwtbJUTpC"}},{"cell_type":"markdown","source":["- Semantic segmentation은 이미지의 픽셀 단위별로 무엇을 지칭하는지를 판단(classification)하는 task입니다.\n","- Semantic segmentation에서 instance(개체)를 구분하는 것까지 추가된 것이 instance segmentation입니다.\n","- U-Net 모델은 Encoder-Decoder 모델에 skip connection을 추가한 모델입니다."],"metadata":{"id":"dqwtNQbVeDeN"}}]}